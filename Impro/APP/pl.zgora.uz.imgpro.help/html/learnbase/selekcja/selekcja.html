<head><meta http-equiv="Content-Type" content="text/html; charset=windows-1250"></head>

<h2>SELECTION</h2>

<h3>Opis</h3>

<p><b>Selekcja cech –</b>  jest niezmiernie wa¿nym etapem w procesie przygotowywania danych do klasyfikacji. 
Przyk³adowo, w wyniku przeprowadzenia eksperymentu z wykorzystaniem p³ytek genowych otrzymujemy informacjê o aktywnoœci tysiêcy lub nawet dziesi¹tków tysiêcy genów - jest to jedna z podstawowych zalet p³ytek genowych. Teoretycznie, du¿a liczba cech, powinna daæ du¿e mo¿liwoœci przy rozró¿nianiu poszczególnych klas. Jednak, jest co najmniej kilka powodów, dla których przeprowadza siê redukcjê dostêpnych cech do niezbêdnego minimum, oto niektóre z nich: 
du¿a liczba cech zwiêksza z³o¿onoœæ obliczeniow¹
wiêkszoœæ z cech jest wzajemnie skorelowana
du¿a liczba cech bêdzie pogarszaæ generalizacjê klasyfikatora - udowodnione jest, ¿e wy¿szy stosunek liczby próbek w zbiorze treningowym, do liczby wolnych parametrów klasyfikatora powoduje polepszenie generalizacji. Natomiast du¿a liczba cech bezpoœrednio przek³ada siê na du¿¹ liczbê parametrów klasyfikatora (np. wagi w klasyfikatorze liniowym, synaptyczne wagi w sieciach neuronowych)
du¿a liczba cech wp³ynie na pogorszenie estymacji b³êdu klasyfikatora</p>
<br>
<br>



<center><p><b>Definicja</b></p>
<p>Bior¹c pod uwagê zestaw funkcji X ={xi; i=1..N} znaleŸæ podzbiór YM = {xj1,xi2,...,xiM} ,gdzie M mniejsze ni¿ N ,który optymalizuje obiektywn¹ funkcja J (Y).</p></center>

<center><img src="Selekcja.jpg" alt="Forward Selection" /></center>
<br>
<br>
<br>
<br>

<br>
<p><h3>Funkcja celu –</h3> </p>
<p>
Jest to funkcja, oceniaj¹ca podzbiory kandydatów i zwracaj¹ca miare ich "dobroci".Funkcje celu s¹ podzielone na dwie grupy: </P>
<p>
<p>Filtry-
Funkcja celu ocenia podzbiory cech przez zawarte w niech informacyje, typowe miêdzyklasowych odleg³oœci, statystyczne zale¿noœci lub œrodki informacyjno-teoretyczny. </P>

<P>Wrappery-
Funkcja celu tworzy klasyfikator wzóru, który ocenia podzbiory funkcji przez ich predykcyjn¹ dok³adnoœæ (ocena rozpoznania na danych testowych) przez statystyczny resampling lub kroswalidacjê.</P>
</p>
<br>
<br>

<p><h3>Strategie przeszukiwania dzielimy na:</h3></p> 

<b> Algorytmy wyk³adnicze</b> 
<p> Te algorytmy oceniaj¹ liczbê podzbiorów, które wyk³adniczo wraz z wymiarowoœci¹ obszaru przeszukiwañ. Najbardziej znane algorytmy to:
 Exhaustive Search, Branch and Bound, Approximate Monotonicity with Branch and Bound, Beam Search</p>
<br>
<b> Algorytmy stochastyczne</b> 
<p>Algorytmy te, zwane tak¿e losowymi, uwzglêdniaj¹ losowoœæ podczas procedury przeszukiwania, aby unikn¹c minimów lokalnych. Najbardzej znane przyk³ady to:
 Random Generation plus Sequential Selection, Simulated Annealing, Algorytmy genetyczne.
</p>

<p>Algorytm genetyczny - rodzaj algorytmu przeszukuj¹cego przestrzeñ alternatywnych rozwi¹zañ problemu w celu wyszukania rozwi¹zañ najlepszych.
Sposób dzia³ania algorytmów genetycznych nieprzypadkowo przypomina zjawisko ewolucji biologicznej. </p>

<br>
<b> Algorytmy sekwencyjne</b> 
<p>Ich dzia³anie opiera siê na dodawaniu i usuwaniu cech sekwencyjnie, niestety maj¹ one tendencje do stawania w minimach lokalnych. Najbardzej znane przyk³ady:
 Sequential Forward Selection(przyk³ad opisany poni¿ej), Sequential Backward Selection
 Plus-l Minus-r Selection, Bidirectional Search, Sequential Floating Selection.
</p>

<p>Sekwencyjna selekcja postêpuj¹ca (SFS - Sequential forward selection) - algorytm polegaj¹cy na wyborze najlepszej dyskryminuj¹cej cechy a nastêpnie dobieraniu kolejnych cech tworz¹cych podzbiory o najwiêkszej wartoœci miary dyskryminacji.</p>
<p> Schemat postêpowania: </p>
<p>a) wyznaczamy wartoœæ miary dyskryminacji dla ka¿dej cechy osobno, a nastêpnie 
wybieramy cechê daj¹c¹ najwiêksz¹ wartoœæ tej miary.</p>
<p>b) wyznaczamy wartoœæ miary dyskryminacji dla wszystkich mo¿liwych dwu 
wymiarowych wektorów zawieraj¹cych cechê wybran¹ w  poprzednim punkcie. 
Wybieramy wektor daj¹cy najwiêksz¹ wartoœæ miary dyskryminacji.</p>  
<p>c) kontynuujemy dodawanie kolejnych cech, a¿ do momentu wybrania p cech.</p>
<br>
<br>
<h3><p> Zestawienie cech </p></h3> 

<b> Algorytmy stochastyczne</b> 
<p>Dok³aoœæ: Zawsze znajduj¹ optymalne rozwi¹zanie
</p>
<p>Z³o¿onoœæ: Wyk³adnicza
</p>
<p>Zalety: Wysokoa dok³adnoœæ
</p>
<p>Wady: Wysoka z³o¿onoœæ
</p>
<br>

<b> Algorytmy Sekwencyje</b> 
<p>Dok³aoœæ: Dobra
</p>
<p>Z³o¿onoœæ: Kwadratowa
</p>
<p>Zalety: Szyki i prosty
</p>
<p>Wady: Brak backtrakingu
</p>
<br>

<b> Algorytmy stochastyczne</b> 
<p>Dok³aoœæ: Dobra przy dobrze dobrancyh parametrach
</p>
<p>Z³o¿onoœæ: Niska
</p>
<p>Zalety: Stworzone aby ucieæ z minimów lokalnych
</p>
<p>Wady: Nie jest ³¹two dobraæ odpowiednie parametry
</p>
<br>

<h4>References</h4>
<a href>http://en.wikipedia.org/wiki/Feature_selection</a href>




